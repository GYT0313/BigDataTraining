# spark 常规 配置   不包括 流式处理的 配置

#################### 全局  #############################
#在用户没有指定时，用于分布式随机操作(groupByKey,reduceByKey等等)的默认的任务数（ shuffle过程中 task的个数 ）。
#默认为 8
spark.default.parallelism=16

#Spark用于缓存的内存大小所占用的Java堆的比率。这个不应该大于JVM中老年代所分配的内存大小，
#默认情况下老年代大小是堆大小的2/3，但是你可以通过配置你的老年代的大小，然后再去增加这个比率。
#默认为 0.66
#spark 1.6 后 过期
#spark.storage.memoryFraction=0.66

#在spark1.6.0版本默认大小为： (“Java Heap” – 300MB) * 0.75。
#例如：如果堆内存大小有4G，将有2847MB的Spark Memory,Spark Memory=(4*1024MB-300)*0.75=2847MB。
#这部分内存会被分成两部分：Storage Memory和Execution Memory，
#而且这两部分的边界由spark.memory.storageFraction参数设定，默认是0.5即50%。
#新的内存管理模型中的优点是，这个边界不是固定的，在内存压力下这个边界是可以移动的。
#如一个区域内存不够用时可以从另一区域借用内存。
spark.memory.fraction=0.75
spark.memory.storageFraction=0.5

#是否要压缩序列化的RDD分区（比如，StorageLevel.MEMORY_ONLY_SER）。
#在消耗一点额外的CPU时间的代价下，可以极大的提高减少空间的使用。
#默认为 false
spark.rdd.compress=true

#The codec used to compress internal data such as RDD partitions,
#broadcast variables and shuffle outputs. By default,
#Spark provides three codecs: lz4, lzf, and snappy. You can also use fully qualified class names to specify the codec,
#e.g.
# 1. org.apache.spark.io.LZ4CompressionCodec,
# 2. org.apache.spark.io.LZFCompressionCodec,
# 3. org.apache.spark.io.SnappyCompressionCodec.   default
spark.io.compression.codec=org.apache.spark.io.SnappyCompressionCodec

#Block size (in bytes) used in Snappy compression,
#in the case when Snappy compression codec is used.
#Lowering this block size will also lower shuffle memory usage when Snappy is used.
#default : 32K
spark.io.compression.snappy.blockSize=32768

#同时获取每一个分解任务的时候，映射输出文件的最大的尺寸（以兆为单位）。
#由于对每个输出都需要我们去创建一个缓冲区去接受它，这个属性值代表了对每个分解任务所使用的内存的一个上限值，
#因此除非你机器内存很大，最好还是配置一下这个值。
#默认48
spark.reducer.maxSizeInFlight=24

#这个配置参数仅适用于HashShuffleMananger的实现，同样是为了解决生成过多文件的问题，
#采用的方式是在不同批次运行的Map任务之间重用Shuffle输出文件，也就是说合并的是不同批次的Map任务的输出数据，
#但是每个Map任务所需要的文件还是取决于Reduce分区的数量，因此，它并不减少同时打开的输出文件的数量，
#因此对内存使用量的减少并没有帮助。只是HashShuffleManager里的一个折中的解决方案。
#默认为false
#spark.shuffle.consolidateFiles=false

#java.io.Externalizable. Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.
#default java.io.Serializable
#spark.serializer=org.apache.spark.serializer.KryoSerializer

#Speculation是在任务调度的时候，如果没有适合当前本地性要求的任务可供运行，
#将跑得慢的任务在空闲计算资源上再度调度的行为，这些参数调整这些行为的频率和判断指标，默认是不使用Speculation的
#默认为false
#  慎用   可能导致数据重复的现象
#spark.speculation=true

#task失败重试次数
#默认为4
spark.task.maxFailures=8

#Spark 是有任务的黑名单机制的，但是这个配置在官方文档里面并没有写，可以设置下面的参数，
#比如设置成一分钟之内不要再把任务发到这个 Executor 上了，单位是毫秒。
#spark.scheduler.executorTaskBlacklistTime=60000

#超过这个时间，可以执行 NODE_LOCAL 的任务
#默认为 3000
spark.locality.wait.process=1000

#超过这个时间，可以执行 RACK_LOCAL 的任务
#默认为 3000
spark.locality.wait.node=1000

#超过这个时间，可以执行 ANY 的任务
#默认为 3000
spark.locality.wait.rack=1000

#################### yarn  ###########################

# 提交的jar文件  的副本数
#默认为 3
spark.yarn.submit.file.replication=1

#container中的线程数
#默认为 25
spark.yarn.containerLauncherMaxThreads=25

#解决yarn-cluster模式下 对处理  permGen space oom异常很有用
#spark.yarn.am.extraJavaOptions=
#spark.driver.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024M

#对象指针压缩 和 gc日志收集打印
#spark.executor.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024M -XX:MaxDirectMemorySize=1536M -XX:+UseCompressedOops -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
#-XX:-UseGCOverheadLimit
#GC默认情况下有一个限制，默认是GC时间不能超过2%的CPU时间，但是如果大量对象创建（在Spark里很容易出现，代码模式就是一个RDD转下一个RDD），
#就会导致大量的GC时间，从而出现OutOfMemoryError: GC overhead limit exceeded，可以通过设置-XX:-UseGCOverheadLimit关掉它。
#-XX:+UseCompressedOops  可以压缩指针（8字节变成4字节）
spark.executor.extraJavaOptions=-XX:PermSize=512M -XX:MaxPermSize=1024m -XX:+CMSClassUnloadingEnabled -Xmn512m -XX:MaxTenuringThreshold=15 -XX:-UseGCOverheadLimit -XX:+UseConcMarkSweepGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:+UseCompressedOops -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log -XX:+HeapDumpOnOutOfMemoryError

#当shuffle缓存的数据超过此值  强制刷磁盘  单位为 byte
#spark.shuffle.spill.initialMemoryThreshold=671088640

################### AKKA 相关 ##########################

#在控制面板通信（序列化任务和任务结果）的时候消息尺寸的最大值，单位是MB。
#如果你需要给驱动器发回大尺寸的结果（比如使用在一个大的数据集上面使用collect()方法），那么你就该增加这个值了。
#默认为 10
spark.akka.frameSize=1024

#用于通信的actor线程数量。如果驱动器有很多CPU核心，那么在大集群上可以增大这个值。
#默认为 4
spark.akka.threads=8

#Spark节点之间通信的超时时间，以秒为单位
#默认为20s
spark.akka.timeout=120

#exector的堆外内存（不会占用 分配给executor的jvm内存）
#spark.yarn.executor.memoryOverhead=2560